{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cyclegan.ipynb","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1217826,"sourceType":"datasetVersion","datasetId":298806},{"sourceId":3956508,"sourceType":"datasetVersion","datasetId":850761}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2019 The TensorFlow Authors.","metadata":{"id":"v1CUZ0dkOo_F"}},{"cell_type":"markdown","source":"## Set up the input pipeline","metadata":{"id":"e1_Y75QXJS6h"}},{"cell_type":"markdown","source":"Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator.","metadata":{"id":"5fGHWOKPX4ta"}},{"cell_type":"code","source":"pip install git+https://github.com/tensorflow/examples.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom zipfile import ZipFile\nfrom PIL import Image\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# import tensorflow_addons as tfa\n\nAUTOTUNE = tf.data.AUTOTUNE\n\nimport numpy as np, pandas as pd, os\nimport matplotlib.pyplot as plt, cv2\nimport tensorflow as tf, re, math","metadata":{"id":"YfIk2es3hJEd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input Pipeline\n\nThis tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/catalog/cycle_gan). \n\nAs mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n\nThis is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n\n* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n* In random mirroring, the image is randomly flipped horizontally i.e., left to right.","metadata":{"id":"iYn4MdZnKCey"}},{"cell_type":"code","source":"# import os\n# import random\n# import tensorflow_datasets as tfds\n\n# # Converting Kaggle dataset to TFDS format\n\n# landscape_dir = '/kaggle/input/landscape-pictures/'\n# vangogh_dir = '/kaggle/input/van-gogh-paintings/VincentVanGogh/'\n\n# # Check if the directories exist\n# if not os.path.exists(landscape_dir):\n#     raise FileNotFoundError(f\"Directory '{landscape_dir}' not found.\")\n# if not os.path.exists(vangogh_dir):\n#     raise FileNotFoundError(f\"Directory '{vangogh_dir}' not found.\")\n\n# train_ratio = 0.8\n\n# landscape_images = [os.path.join(landscape_dir, filename) for filename in os.listdir(landscape_dir)]\n\n# vangogh_images = [os.path.join(vangogh_dir, filename) for filename in os.listdir(vangogh_dir)]\n\n# # Randomly shuffle images\n# random.shuffle(landscape_images)\n# random.shuffle(vangogh_images)\n\n# # Split the images into training and testing sets\n# num_train_ln = int(len(landscape_images) * train_ratio)\n# num_train_vg = int(len(vangogh_images) * train_ratio)\n\n# train_ln_files = landscape_images[:num_train_ln]\n# test_ln_files = landscape_images[num_train_ln:]\n\n# train_vg_files = vangogh_images[:num_train_vg]\n# test_vg_files = vangogh_images[num_train_vg:]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndataset, metadata = tfds.load('cycle_gan/vangogh2photo', with_info=True, as_supervised=True)\n\ntrain_vg, train_ln = dataset['trainA'], dataset['trainB']\ntest_vg, test_ln = dataset['testA'], dataset['testB']\n\n\n# kaggle_dataset = pd.read_csv('/kaggle/input/van-gogh-paintings/VanGoghPaintings.csv')\n\n# # Split the Kaggle dataset into train and test sets\n# train_kaggle, test_kaggle = train_test_split(kaggle_dataset, test_size=0.2, random_state=42)\n\n# # Convert the train and test sets into TensorFlow Datasets\n# tf_train_kaggle = tf.data.Dataset.from_tensor_slices(train_kaggle)\n# tf_test_kaggle = tf.data.Dataset.from_tensor_slices(test_kaggle)\n\n# # Combine the Kaggle dataset with train_vg and test_vg\n# train_vg_combined = train_vg.concatenate(tf_kaggle_dataset)\n# test_vg_combined = test_vg.concatenate(tf_kaggle_dataset)\n\n\n\n# # Select only the image paths from the DataFrame\n# image_paths = kaggle_dataset['image_path'].tolist()\n\n# # Split the image paths into train and test sets\n# train_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n\n# # Convert the train and test sets into TensorFlow Datasets\n# tf_train_kaggle = tf.data.Dataset.from_tensor_slices(train_paths)\n# tf_test_kaggle = tf.data.Dataset.from_tensor_slices(test_paths)\n\n# # Combine the Kaggle dataset with train_vg and test_vg\n# train_vg_combined = train_vg.concatenate(tf_train_kaggle)\n# test_vg_combined = test_vg.concatenate(tf_test_kaggle)\n\n","metadata":{"id":"iuGVPOo7Cce0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"id":"2CbTEt448b4R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image","metadata":{"id":"Yn3IwqhiIszt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image","metadata":{"id":"muhR2cgbLKWW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image\n\n\n# def random_jitter(image):\n#     # Randomly resize the image\n#     image = tf.image.resize(image, [286, 286])\n\n#     # Random crop to 256x256\n#     image = tf.image.random_crop(image, size=[256, 256, 3])\n\n#     # Randomly flip the image horizontally\n#     image = tf.image.random_flip_left_right(image)\n\n#     return image\n","metadata":{"id":"fVQOjcPVLrUc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_train(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image\n\ndef preprocess_image_test(image, label):\n  image = normalize(image)\n  return image","metadata":{"id":"tyaP4hLJ8b4W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_tfdataset_train(dataset):\n    return dataset.map(lambda x, y: x).map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\ndef preprocess_tfdataset_test(dataset):\n    return dataset.map(lambda x, y: x).map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n# Functions for loading in the flickr scraped cityscape images\ndef process_path_train(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = preprocess_image_train(img)\n    return img\n\ndef process_path_test(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = preprocess_image_test(img)\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # cityscape train, test preprocessing\n# train_ln = train_ln.map(process_path_train, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n# test_ln = test_ln.map(process_path_test, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n\n# # train_pic = preprocess_tfdataset_train(train_pic)\n# # test_pic = preprocess_tfdataset_test(test_pic)\n# train_vg = preprocess_tfdataset_train(train_vg)\n# test_vg = preprocess_tfdataset_test(test_vg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ln = tf.data.Dataset.from_tensor_slices(train_ln_files)\n# test_ln = tf.data.Dataset.from_tensor_slices(test_ln_files)\n# train_vg = tf.data.Dataset.from_tensor_slices(train_vg_files)\n# test_vg = tf.data.Dataset.from_tensor_slices(test_vg_files)\n\ntrain_vg = train_vg.cache().map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntrain_ln = train_ln.cache().map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntest_vg = test_vg.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntest_ln = test_ln.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\n\n# # Apply preprocessing functions to the datasets\n# train_vg = train_vg.map(lambda x: preprocess_image_train(x, label), num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n# train_ln = train_ln.map(lambda x: preprocess_image_train(x, label), num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n# test_vg = test_vg.map(lambda x: preprocess_image_test(x, label), num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n# test_ln = test_ln.map(lambda x: preprocess_image_test(x, label), num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"id":"RsajGXxd5JkZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_vg = next(iter(train_vg))\nsample_ln = next(iter(train_ln))","metadata":{"id":"e3MhJ3zVLPan","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Van Gogh Painting')\nplt.imshow(sample_vg[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Van Gogh Painting with random jitter')\nplt.imshow(random_jitter(sample_vg[0]) * 0.5 + 0.5)","metadata":{"id":"4pOYjMk_KfIB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Landscape')\nplt.imshow(sample_ln[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Landscape with random jitter')\nplt.imshow(random_jitter(sample_ln[0]) * 0.5 + 0.5)","metadata":{"id":"0KJyB9ENLb2y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import and reuse the Pix2Pix models","metadata":{"id":"hvX8sKsfMaio"}},{"cell_type":"markdown","source":"Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n\nThe model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n\n* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.\n\nThere are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n\n* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n\n![Cyclegan model](images/cyclegan_model.png)","metadata":{"id":"cGrL73uCd-_M"}},{"cell_type":"code","source":"# # Making the functions\n# def _get_norm_layer(norm):\n#   if norm == \"none\":\n#     return lambda: lambda x: x\n#   elif norm == \"batch_norm\":\n#     return tf.keras.layers.BatchNormalization\n#   elif norm == \"instance_norm\":\n#     return tfa.layers.InstanceNormalization\n#   elif norm == \"layer_norm\":\n#     return tf.keras.layers.LayerNormalization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Making the resnet block\n# def ResnetGenerator(input_shape=(256, 256, 3), output_channels=3, dim=64,\n#                     n_downsamplings=2, n_blocks=9, norm='instance_norm'):\n#     Norm = _get_norm_layer(norm)\n    \n#     def _residual_block(x):\n#         dim = x.shape[-1]\n#         h = x\n#         h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n#         h = tf.keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n#         h = Norm()(h)\n#         h = tf.nn.relu(h)\n#         h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n#         h = tf.keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n#         h = Norm()(h)\n#         return tf.keras.layers.add([x, h])\n# # 0\n#     h = inputs = tf.keras.Input(shape=input_shape)\n# # 1\n#     h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n#     h = tf.keras.layers.Conv2D(dim, 7, padding='valid', use_bias=False)(h)\n#     h = Norm()(h)\n#     h = tf.nn.relu(h)\n# # 2\n#     for _ in range(n_downsamplings):\n#         dim *= 2\n#         h = tf.keras.layers.Conv2D(dim, 3, strides=2, padding='same', use_bias=False)(h)\n#         h = Norm()(h)\n#         h = tf.nn.relu(h)\n# # 3\n#     for _ in range(n_blocks):\n#         h = _residual_block(h)\n# # 4\n#     for _ in range(n_downsamplings):\n#         dim //= 2\n#         h = tf.keras.layers.Conv2DTranspose(dim, 3, strides=2, padding='same', use_bias=False)(h)\n#         h = Norm()(h)\n#         h = tf.nn.relu(h)\n# # 5\n#     h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n#     h = tf.keras.layers.Conv2D(output_channels, 7, padding='valid')(h)\n#     h = tf.tanh(h)\n    \n#     return tf.keras.Model(inputs=inputs, outputs=h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def ConvDiscriminator(input_shape=(256, 256, 3), dim=64, n_downsamplings=3, norm='instance_norm'):\n#     dim_ = dim\n#     Norm = _get_norm_layer(norm)\n# # 0\n#     h = inputs = tf.keras.Input(shape=input_shape)\n# # 1\n#     h = tf.keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n#     h = tf.nn.leaky_relu(h, alpha=0.2)\n#     for _ in range(n_downsamplings - 1):\n#         dim = min(dim * 2, dim_ * 8)\n#         h = tf.keras.layers.Conv2D(dim, 4, strides=2, padding='same', use_bias=False)(h)\n#         h = Norm()(h)\n#         h = tf.nn.leaky_relu(h, alpha=0.2)\n#     # 2\n#         dim = min(dim * 2, dim_ * 8)\n#         h = tf.keras.layers.Conv2D(dim, 4, strides=1, padding='same', use_bias=False)(h)\n#         h = Norm()(h)\n#         h = tf.nn.leaky_relu(h, alpha=0.2)\n#     # 3\n#         h = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same')(h)\n#     return tf.keras.Model(inputs=inputs, outputs=h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install keras","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from mediapipe_model_maker import image_classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\n# import tensorflow_addons as tfa\n\ngenerator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\ngenerator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\ndiscriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\ndiscriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)\n\n# ## Building the Resnet Generator as per the cycle gan paper\n# generator_g = ResnetGenerator()\n# generator_f = ResnetGenerator()\n# ## Building the Discriminator as per the cucle gan paper\n# discriminator_x = ConvDiscriminator()\n# discriminator_y = ConvDiscriminator()","metadata":{"id":"8ju9Wyw87MRW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_vg = generator_g(sample_vg)\n# to_ln = generator_f(sample_ln)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\n# imgs = [sample_vg, to_ln, sample_ln, to_vg]\n# title = ['Van Gogh Painting', 'To Landscape', 'Landscape', 'To Van Gogh Painting']\n\nimgs = [sample_ln, to_vg]\ntitle = ['Landscape', 'To Van Gogh Painting']\n\nfor i in range(len(imgs)):\n  plt.subplot(2, 2, i+1)\n  plt.title(title[i])\n  if i % 2 == 0:\n    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n  else:\n    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()","metadata":{"id":"wDaGZ3WpZUyw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.title('Is a real Landscape?')\nplt.imshow(discriminator_y(sample_ln)[0, ..., -1], cmap='RdBu_r')\n\nplt.subplot(122)\nplt.title('Is a real Van Gogh Painting?')\nplt.imshow(discriminator_x(sample_vg)[0, ..., -1], cmap='RdBu_r')\n\nplt.show()","metadata":{"id":"O5MhJmxyZiy9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss functions","metadata":{"id":"0FMYgY_mPfTi"}},{"cell_type":"markdown","source":"In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n\nThe discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#build_the_generator).","metadata":{"id":"JRqt02lupRn8"}},{"cell_type":"code","source":"LAMBDA = 12\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\ndef discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5\n\ndef generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)\n\n\ndef calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  \n  return LAMBDA * loss1\n\n\ndef identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss","metadata":{"id":"wkMNfBWlT-PV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n\nIn cycle consistency loss, \n\n* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n\n$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n\n$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n\n\n![Cycle loss](images/cycle_loss.png)","metadata":{"id":"5iIWQzVF7f9e"}},{"cell_type":"code","source":"# def calc_cycle_loss(real_image, cycled_image):\n#   loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  \n#   return LAMBDA * loss1","metadata":{"id":"NMpVGj_sW6Vo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n\nIf you run the zebra-to-horse model on a horse or the horse-to-zebra model on a zebra, it should not modify the image much since the image already contains the target class.\n\n$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$","metadata":{"id":"U-tJL-fX0Mq7"}},{"cell_type":"code","source":"# def identity_loss(real_image, same_image):\n#   loss = tf.reduce_mean(tf.abs(real_image - same_image))\n#   return LAMBDA * 0.5 * loss","metadata":{"id":"05ywEH680Aud","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize the optimizers for all the generators and the discriminators.","metadata":{"id":"G-vjRM7IffTT"}},{"cell_type":"code","source":"generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"id":"iWCn_PVdEJZ7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checkpoints","metadata":{"id":"aKUZnDiqQrAh"}},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\n\nckpt = tf.train.Checkpoint(generator_g=generator_g,\n                           generator_f=generator_f,\n                           discriminator_x=discriminator_x,\n                           discriminator_y=discriminator_y,\n                           generator_g_optimizer=generator_g_optimizer,\n                           generator_f_optimizer=generator_f_optimizer,\n                           discriminator_x_optimizer=discriminator_x_optimizer,\n                           discriminator_y_optimizer=discriminator_y_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')","metadata":{"id":"WJnftd5sQsv6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 100\n\ndef generate_images(model, test_input):\n  prediction = model(test_input)\n    \n  plt.figure(figsize=(12, 12))\n\n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Image']\n\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()","metadata":{"id":"RmdVsmvhPxyy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though the training loop looks complicated, it consists of four basic steps:\n\n* Get the predictions.\n* Calculate the loss.\n* Calculate the gradients using backpropagation.\n* Apply the gradients to the optimizer.","metadata":{"id":"kE47ERn5fyLC"}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n    \n    fake_y = generator_g(real_x, training=True)\n    cycled_x = generator_f(fake_y, training=True)\n\n    fake_x = generator_f(real_y, training=True)\n    cycled_y = generator_g(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_f(real_x, training=True)\n    same_y = generator_g(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_y)\n    gen_f_loss = generator_loss(disc_fake_x)\n    \n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n    \n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n  \n  # Calculate the gradients for generator and discriminator\n  generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n  generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n  \n  discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n  discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n  \n  # Apply the gradients to the optimizer\n  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n  \n  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n  \n  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","metadata":{"id":"KBKUV2sKXDbY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS // 5):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((train_ln, train_vg)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n += 1\n\n  #clear_output(wait=True)\n  if epoch % 50 == 0: print(\"EPOCH:\", epoch)\n  generate_images(generator_g, sample_ln)\n\n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n\n  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))","metadata":{"id":"2M7LmLtGEMQJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate using test dataset","metadata":{"id":"1RGysMU_BZhx"}},{"cell_type":"code","source":"for inp in test_ln.take(EPOCHS // 5):\n    generate_images(generator_g, inp)","metadata":{"id":"KUgSnmy2nqSP","trusted":true},"execution_count":null,"outputs":[]}]}